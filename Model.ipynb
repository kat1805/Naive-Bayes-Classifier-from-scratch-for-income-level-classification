{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import math \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should prepare the data by reading it from a file and converting it into a useful format for training and testing\n",
    "# and implement 90-10 splitting as specified in the project description.\n",
    "def preprocess(filename):\n",
    "\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    train_data, test_data = train_test_split(data, test_size = 0.10, shuffle = False)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lessthan(df_lessthan, numeric_att):\n",
    "        numeric = {}\n",
    "        nominal = {}\n",
    "        \n",
    "        for cols in df_lessthan.columns[0:-1]:\n",
    "            if cols not in numeric_att:\n",
    "                likelihood = (df_lessthan.groupby(cols).size())/len(df_lessthan)\n",
    "                nominal[cols] = likelihood\n",
    "            else:\n",
    "                mean = df_lessthan[cols].mean()\n",
    "                sd = df_lessthan[cols].std()\n",
    "                numeric[cols] = ([mean, sd])\n",
    "                \n",
    "        return numeric, nominal\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morethan(df_morethan, numeric_att):\n",
    "        numeric = {}\n",
    "        nominal = {}\n",
    "        \n",
    "        for cols in df_morethan.columns[0:-1]:\n",
    "            if cols not in numeric_att:\n",
    "                likelihood = (df_morethan.groupby(cols).size())/len(df_morethan)\n",
    "                nominal[cols] = likelihood\n",
    "            else:\n",
    "                mean = df_morethan[cols].mean()\n",
    "                sd = df_morethan[cols].std()\n",
    "                numeric[cols] = ([mean, sd])\n",
    "           \n",
    "        return numeric, nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculat prior probabilities and likelihoods (conditional probabilities) from the training data and using\n",
    "# to build a naive Bayes model\n",
    "\n",
    "def train(train_data, test_data):    \n",
    "    \n",
    "    prior = {}\n",
    "    \n",
    "    posterior_lessthan_nom = {}\n",
    "    posterior_greaterthan_nom = {}\n",
    "    posterior_lessthan_num = {}\n",
    "    posterior_greaterthan_num = {}\n",
    "    \n",
    "    \n",
    "    file = pd.read_csv(filename)\n",
    "    \n",
    "    y_actual = test_data['label']\n",
    "    test_data = test_data.iloc[:,:-1]\n",
    "   \n",
    "   \n",
    "    prior_prob = (train_data.groupby('label').size())/len(train_data)\n",
    "    \n",
    "    prior[' <=50K'] = prior_prob[0]\n",
    "    prior[' >50K'] = prior_prob[1]\n",
    "    \n",
    "    \n",
    "    df_lessthan = train_data.loc[train_data['label'] == ' <=50K']\n",
    "    df_morethan = train_data.loc[train_data['label'] == ' >50K']\n",
    "\n",
    "    \n",
    "    numeric_att = ['age', 'education num', 'hours per week']\n",
    "   \n",
    "    #calculting the probabilities of numerical and nominal variables\n",
    "    posterior_lessthan_num, posterior_lessthan_nom = lessthan(df_lessthan, numeric_att)\n",
    "    posterior_greaterthan_num, posterior_greaterthan_nom = morethan(df_morethan, numeric_att)\n",
    "    \n",
    "    return df_lessthan, df_morethan, y_actual, test_data, prior_prob, posterior_lessthan_nom, posterior_greaterthan_nom, posterior_lessthan_num, posterior_greaterthan_num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_prob(value, mean, sd):\n",
    "    \n",
    "    denom = 1/(sd*math.sqrt(2*math.pi))\n",
    "    num = math.exp((-1/2)*(((value - mean)/sd)**2))\n",
    "\n",
    "    gaussian = denom * num\n",
    "    \n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict classes for new items in the testing data\n",
    "def predict(y_actual, test_data, prior_prob, lessthan_nom, greaterthan_nom, lessthan_num, greaterthan_num):\n",
    "    \n",
    "    \n",
    "    numeric_att = ['age', 'education num', 'hours per week']\n",
    "    nominal_att = [att for att in test_data.columns if att not in numeric_att]\n",
    "    \n",
    "    y_pred = []\n",
    "    \n",
    "     \n",
    "    total_lessthan = 0\n",
    "    total_morethan = 0\n",
    "    \n",
    "   \n",
    "    \n",
    "        \n",
    "    for i in test_data.index:\n",
    "        for col in numeric_att:\n",
    "            total_lessthan += math.log(gaussian_prob(test_data[col][i], lessthan_num[col][0], lessthan_num[col][1]))\n",
    "            total_morethan += math.log(gaussian_prob(test_data[col][i], greaterthan_num[col][0], greaterthan_num[col][1]))\n",
    "\n",
    "                \n",
    "        for cols in nominal_att:\n",
    "\n",
    "            val_1 = lessthan_nom[cols]\n",
    "            \n",
    "            if np.finfo('float64').eps < (1/len(df_lessthan)):\n",
    "                epsilon_less = np.finfo('float64').eps\n",
    "            else:\n",
    "                epsilon_less = 0\n",
    "                \n",
    "            if test_data[cols][i] in (df_lessthan.groupby([cols]).size()).index:\n",
    "                vals_1 = val_1[test_data[cols][i]]\n",
    "                total_lessthan += math.log(vals_1)\n",
    "            else:\n",
    "                #prob_less = np.finfo('float64').eps\n",
    "                total_lessthan += math.log(epsilon_less)\n",
    "                \n",
    "                \n",
    "            if np.finfo('float64').eps < (1/len(df_morethan)):\n",
    "                epsilon_more = np.finfo('float64').eps\n",
    "            else:\n",
    "                epsilon_more = 0\n",
    "\n",
    "            val_2 = greaterthan_nom[cols]\n",
    "\n",
    "            if test_data[cols][i] in (df_morethan.groupby([cols]).size()).index:\n",
    "                vals_2 = val_2[test_data[cols][i]]\n",
    "                total_morethan += math.log(vals_2)\n",
    "            else:\n",
    "                #prob_more = np.finfo('float64').eps\n",
    "                total_morethan += math.log(epsilon_more)\n",
    "\n",
    "        final_lessthan = math.log(prior_prob[0]) + total_lessthan\n",
    "        final_morethan = math.log(prior_prob[1]) + total_morethan\n",
    "\n",
    "        y_pred.append([final_lessthan, final_morethan])\n",
    "\n",
    "\n",
    "        total_lessthan = 0\n",
    "        total_morethan = 0\n",
    "    \n",
    " \n",
    "    predicted_class = []\n",
    "    \n",
    "    for row in y_pred:\n",
    "        if row[0]<row[1]:\n",
    "            predicted_class.append('>50K')\n",
    "        elif row[0]>row[1]:\n",
    "            predicted_class.append('<=50K')\n",
    "    return y_pred, predicted_class, y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaliate the prediction performance by comparing your modelâ€™s class outputs to ground\n",
    "# truth labels, return and output accuracy, confusion matrix and F1 score.\n",
    "\n",
    "def evaluate(y_hat, clas, y_actual):\n",
    "\n",
    "    y = [rows for rows in y_actual]\n",
    "\n",
    "    labels = [' <=50K', ' >50K']\n",
    "    labels_2 = ['<=50K', '>50K']\n",
    " \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(clas)): \n",
    "    \n",
    "        if ((clas[i])==(labels_2[0]) and (y[i]==labels[0])):\n",
    "            TP += 1\n",
    "        if ((clas[i])==(labels_2[1]) and (y[i]==labels[1])):\n",
    "            TN += 1\n",
    "        if ((y[i])==(labels[1]) and (clas[i]==labels_2[0])):\n",
    "            FP += 1\n",
    "        if (((y[i])==(labels[0]) and (clas[i]==labels_2[1]))):\n",
    "            FN += 1\n",
    "    \n",
    "    \n",
    "    accuracy = (TP+TN)/len(y_hat)\n",
    "    precision = TP/(TP+FP)\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    F1score = (2*precision*sensitivity)/(sensitivity + precision)\n",
    "    \n",
    "    \n",
    "    confusion_matrix = ([[TP, FN], [FP, TN]])\n",
    " \n",
    "    return confusion_matrix, accuracy, F1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.86\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "          Positive  Negative\n",
      "Positive        69         6\n",
      "Negative         8        17\n",
      "\n",
      "\n",
      "F1score:  0.9078947368421053\n",
      "\n",
      "\n",
      "Attribute vectors of instances [0, 1, 2]:  [[68 ' ?' ' 1st-4th' 2 ' Divorced' ' ?' ' Not-in-family' ' White'\n",
      "  ' Female' 20 ' United-States' ' <=50K']] [[39 ' State-gov' ' Bachelors' 13 ' Never-married' ' Adm-clerical'\n",
      "  ' Not-in-family' ' White' ' Male' 40 ' United-States' ' <=50K']] [[50 ' Self-emp-not-inc' ' Bachelors' 13 ' Married-civ-spouse'\n",
      "  ' Exec-managerial' ' Husband' ' White' ' Male' 13 ' United-States'\n",
      "  ' <=50K']]\n",
      "\n",
      "Number of instances (N):  1000\n",
      "Number of attributes (F):  11\n",
      "Number of labels (L):  2\n",
      "\n",
      "\n",
      "Predicted class log-probabilities for instance N-3:  [-20.71689698193305, -19.556273652832147]\n",
      "Predicted class ID for instance N-3:  >50K\n",
      "\n",
      "Predicted class log-probabilities for instance N-2:  [-25.33907063773019, -22.744589775643146]\n",
      "Predicted class ID for instance N-2:  >50K\n",
      "\n",
      "Predicted class log-probabilities for instance N-1:  [-16.852794958645735, -16.716481989445864]\n",
      "Predicted class ID for instance N-1:  >50K\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full ADULT data set, and print the evaluation results. [0.33 marks]\n",
    "\n",
    "\n",
    "# First, read in the data and apply your NB model to the ADULT data\n",
    "filename = 'adult.csv'\n",
    "\n",
    "train_data, test_data = preprocess(filename)\n",
    "\n",
    "df_lessthan, df_morethan, y_actual, test_data, prior_prob, posterior_lessthan_nom, posterior_greaterthan_nom, posterior_lessthan_num, posterior_greaterthan_num = train(train_data, test_data)\n",
    "\n",
    "y_pred, predicted_class, y_actual = predict(y_actual, test_data, prior_prob, posterior_lessthan_nom, posterior_greaterthan_nom, posterior_lessthan_num, posterior_greaterthan_num)\n",
    "# Second, print the full evaluation results from the evaluate() function\n",
    "\n",
    "confusion_matrix, accuracy, F1score = evaluate(y_pred, predicted_class, y_actual)\n",
    "\n",
    "print('Accuracy: ', accuracy)\n",
    "print('\\n')\n",
    "print('Confusion Matrix:\\n')\n",
    "c = (['Positive',confusion_matrix[0][0], confusion_matrix[1][0]], ['Negative',confusion_matrix[0][1], confusion_matrix[1][1]])\n",
    "matrix = pd.DataFrame(c, columns = [' ', 'Positive', 'Negative'], index = None)\n",
    "print(matrix.to_string(index=False))\n",
    "print('\\n')\n",
    "print('F1score: ', F1score)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of attributes, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "# The \"predicted class ID\" must be in range (0, L)\n",
    "\n",
    "print(\"Attribute vectors of instances [0, 1, 2]: \", train_data.loc[[0]].to_numpy(), train_data.loc[[1]].to_numpy(), train_data.loc[[2]].to_numpy()) # of the first three records in adult.csv\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", len(test_data) +len(train_data) )\n",
    "print(\"Number of attributes (F): \", len(test_data.columns) )\n",
    "print(\"Number of labels (L): \", len(set(predicted_class)))\n",
    "\n",
    "\n",
    "# print out the prediction results of the last three instances\n",
    "print(\"\\n\\nPredicted class log-probabilities for instance N-3: \", y_pred[len(y_pred)-3])\n",
    "print(\"Predicted class ID for instance N-3: \", predicted_class[len(predicted_class)-3] )\n",
    "print(\"\\nPredicted class log-probabilities for instance N-2: \", y_pred[len(y_pred)-2])\n",
    "print(\"Predicted class ID for instance N-2: \",predicted_class[len(predicted_class)-2] )\n",
    "print(\"\\nPredicted class log-probabilities for instance N-1: \", y_pred[len(y_pred)-1])\n",
    "print(\"Predicted class ID for instance N-1: \", predicted_class[len(predicted_class)-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity:  0.8961038961038961\n",
      "Specificity:  0.7391304347826086\n"
     ]
    }
   ],
   "source": [
    "#Calculting sensitivity and specificity\n",
    "\n",
    "TP = confusion_matrix[0][0]\n",
    "FN = confusion_matrix[0][1]\n",
    "FP = confusion_matrix[1][0]\n",
    "TN = confusion_matrix[1][1]\n",
    "\n",
    "sensitivity = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP)\n",
    "\n",
    "print('Sensitivity: ', sensitivity)\n",
    "print('Specificity: ', specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model details:\n",
    "\n",
    " * The orginal dataset was split into training and testing sets (90 -10) split.\n",
    " * Gaussian distribution was used for the conditional probabilities of numeric values.\n",
    " * Strictly positive and very small constant - epsilon is used for the test values which have a conditional probability of zero. \n",
    " \n",
    " \n",
    "The evaluated model is 90% sensitive and 74% specific. The higher value of sensitivity suggests that this model is better at predicting true positive cases than predicting true nagative cases. There is approximately 16% difference between the two in this model. The training dataset has 900 values split according to the class labels as shown below - \n",
    " \n",
    "```        \n",
    "          Total count\n",
    "label        \n",
    " <=50K    692\n",
    " >50K     208\n",
    " ```\n",
    "The above table shows that only 30% of the training values belong to the negative class label showing a bias towards the positive class. This suggests that the lower specificity is caused by the lower amount of negative class values in the training dataset (i.e. the model is learning a smaller number of negative class instances and hence is not able to classify all the negative values in the test dataset). \n",
    "\n",
    "The model performance can be improved by - \n",
    "   * Changing the train-test split to 70-30 or a ratio which gives more training values since the current dataset       is quite small for a 90-10 split\n",
    "   * Using a more balanced dataset\n",
    "   * Removing the correlated features\n",
    "   * Handling text-data by applying cleaning techniques such as stemming, lemmatization\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 part a\n",
    "\n",
    "def KDE_gaussian(testvalue, df, col, kernel_bandwidth):\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for value in df[col]:\n",
    "        total += gaussian_prob(testvalue, value, kernel_bandwidth)\n",
    "   \n",
    "    return ((1/len(df))*total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_KDE(df_lessthan, df_morethan, sigma, y_actual, test_data, prior_prob, lessthan_nom, greaterthan_nom, lessthan_num, greaterthan_num):\n",
    "    \n",
    "    \n",
    "    numeric_att = ['age', 'education num', 'hours per week']\n",
    "    nominal_att = [att for att in test_data.columns if att not in numeric_att]\n",
    "    \n",
    "    \n",
    "    y_pred = []\n",
    "    \n",
    "     \n",
    "    total_lessthan = 0\n",
    "    total_morethan = 0\n",
    "    \n",
    "  \n",
    "\n",
    "    for i in test_data.index:\n",
    "        \n",
    "        for col in numeric_att:\n",
    "            \n",
    "            total_lessthan += math.log(KDE_gaussian(test_data[col][i], df_lessthan, col, sigma))\n",
    "            total_morethan += math.log(KDE_gaussian(test_data[col][i], df_morethan, col, sigma))\n",
    "        \n",
    "        for cols in nominal_att:\n",
    "\n",
    "            val_1 = lessthan_nom[cols]\n",
    "            \n",
    "            if np.finfo('float64').eps < (1/len(df_lessthan)):\n",
    "                epsilon_less = np.finfo('float64').eps\n",
    "            else:\n",
    "                epsilon_less = 0\n",
    "                \n",
    "            if test_data[cols][i] in (df_lessthan.groupby([cols]).size()).index:\n",
    "                vals_1 = val_1[test_data[cols][i]]\n",
    "                total_lessthan += math.log(vals_1)\n",
    "            else:\n",
    "                #prob_less = np.finfo('float64').eps\n",
    "                total_lessthan += math.log(epsilon_less)\n",
    "                \n",
    "                \n",
    "            if np.finfo('float64').eps < (1/len(df_morethan)):\n",
    "                epsilon_more = np.finfo('float64').eps\n",
    "            else:\n",
    "                epsilon_more = 0\n",
    "\n",
    "            val_2 = greaterthan_nom[cols]\n",
    "\n",
    "            if test_data[cols][i] in (df_morethan.groupby([cols]).size()).index:\n",
    "                vals_2 = val_2[test_data[cols][i]]\n",
    "                total_morethan += math.log(vals_2)\n",
    "            else:\n",
    "                #prob_more = np.finfo('float64').eps\n",
    "                total_morethan += math.log(epsilon_more)\n",
    "\n",
    "        final_lessthan = math.log(prior_prob[0]) + total_lessthan\n",
    "        final_morethan = math.log(prior_prob[1]) + total_morethan\n",
    "\n",
    "        y_pred.append([final_lessthan, final_morethan])\n",
    "\n",
    "\n",
    "        total_lessthan = 0\n",
    "        total_morethan = 0\n",
    " \n",
    "    predicted_class = []\n",
    "    for row in y_pred:\n",
    "        if row[0]<row[1]:\n",
    "            predicted_class.append('>50K')\n",
    "        elif row[0]>row[1]:\n",
    "            predicted_class.append('<=50K')\n",
    "    return y_pred, predicted_class, y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandwidth_selection(df_lessthan, df_morethan, y_actual, test_data, prior_prob, lessthan_nom, greaterthan_nom, lessthan_num, greaterthan_num):\n",
    "    \n",
    "\n",
    "    bandwidth = [3,5,7,9,15]\n",
    "    max_accuracy = {}\n",
    "    maxval = 0\n",
    "    band = 0\n",
    "    for val in bandwidth:\n",
    "        y_pred, predicted_class, y_actual = predict_with_KDE(df_lessthan, df_morethan, val, y_actual, test_data, prior_prob, lessthan_nom, greaterthan_nom, lessthan_num, greaterthan_num)\n",
    "        confusion_matrix, accuracy, F1score = evaluate(y_pred, predicted_class, y_actual)\n",
    "        max_accuracy[str(accuracy)] = [F1score, confusion_matrix]\n",
    "        if maxval == 0:\n",
    "            maxval = accuracy\n",
    "            band = val\n",
    "        elif accuracy > maxval:\n",
    "            maxval = accuracy\n",
    "            band = val\n",
    "            \n",
    "        print(f\" Bandwidth: {val}, Accuracy: {accuracy}, F1score: {F1score}\")\n",
    "        \n",
    "    print(f\" KDE bandwidth: {band}, Max Accuracy: {maxval}, F1score: {max_accuracy[str(maxval)][0]}\")\n",
    "    #return sort(max_accuracy[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bandwidth: 3, Accuracy: 0.86, F1score: 0.9066666666666665\n",
      " Bandwidth: 5, Accuracy: 0.81, F1score: 0.8689655172413794\n",
      " Bandwidth: 7, Accuracy: 0.82, F1score: 0.8767123287671235\n",
      " Bandwidth: 9, Accuracy: 0.83, F1score: 0.8827586206896552\n",
      " Bandwidth: 15, Accuracy: 0.83, F1score: 0.8827586206896552\n",
      " KDE bandwidth: 3, Max Accuracy: 0.86, F1score: 0.9066666666666665\n"
     ]
    }
   ],
   "source": [
    "filename = 'adult.csv'\n",
    "\n",
    "train_data, test_data = preprocess(filename)\n",
    "\n",
    "df_lessthan, df_greaterthan, y_actual, test_data, prior_prob, lessthan_nom, greaterthan_nom, lessthan_num, greaterthan_num = train(train_data, test_data)\n",
    "\n",
    "bandwidth_selection(df_lessthan, df_greaterthan, y_actual, test_data, prior_prob, lessthan_nom, greaterthan_nom, lessthan_num, greaterthan_num)\n",
    "\n",
    "\n",
    "#print(f\" Mean for less than: {lessthan_num}, SD: {lessthan_num}, '\\n', Mean for greater than: {greaterthan_num}, SD: {greaterthan_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "\n",
    "\n",
    "For the current model - \n",
    "\n",
    " * Accuracy without KDE = 86%\n",
    " * Accuracy with KDE = 86%\n",
    "\n",
    "The current dataset (adult.csv) contains 3 numerical variables and 8 categorical variables. Looking at the plots of these columns - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, test_data = preprocess('adult.csv')\n",
    "  \n",
    "figure, axis = plt.subplots(2, 3)\n",
    "  \n",
    "axis[0, 0].hist(train_data['age'])\n",
    "axis[0, 0].set_title(\"Age\")\n",
    "\n",
    "axis[0, 1].hist(train_data['education num'])\n",
    "axis[0, 1].set_title(\"Education num\")\n",
    "  \n",
    "axis[0, 2].hist(train_data['hours per week'], bins=4)\n",
    "axis[0, 2].set_title(\"Hours per week\")\n",
    "\n",
    "plt.savefig('Train data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the numerical columns in the training dataset do not follow a normal distribution except 'Hours of week' which is closer to a Gaussian distribution. But as the number of numerical attributes is only 3 compared to the 8 nominal attributes, this impact is not significant as the log of the probabilities lie between 0-1.\n",
    "\n",
    "For without KDE evalutation, the model loops through a few values for the selection of the kernel bandwidth. In the end the kernel bandwidth with the highest accuracy has been selected. Again just as the Gaussian model,the model takes log of the probabilities; the smaller number of numerical attributes compared to the much larger number of nominal attributes do not impact significantly on the model evaluation resulting into similar accuracies for the with and without KDE evaluations. \n",
    "\n",
    "Hence both the methods are suitable for the model as they are resulting in the same accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-folds =  2\n",
      "Average accuracy =  0.817\n",
      "Average recall =  0.8518507244612201\n",
      "Average senstivity =  0.8518507244612201\n",
      "Average Specificity =  0.7027523623818809\n",
      "\n",
      "\n",
      "k-folds =  10\n",
      "Average accuracy =  0.8119999999999999\n",
      "Average recall =  0.8422969053304696\n",
      "Average senstivity =  0.8422969053304696\n",
      "Average Specificity =  0.7127698935525022\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 2 part b\n",
    "\n",
    "filename = 'adult.csv'\n",
    "\n",
    "data = pd.read_csv(filename)\n",
    "#print(data)\n",
    "dat1 = np.array(data)\n",
    "#print(dat1)\n",
    "\n",
    "total_accuracy = 0\n",
    "total_recall = 0\n",
    "total_sensitivity = 0\n",
    "total_specificity = 0\n",
    "\n",
    "folds = [2, 10]\n",
    "\n",
    "for fold in folds:\n",
    "    \n",
    "    kfold = KFold(n_splits = fold, random_state=None, shuffle=False)\n",
    "\n",
    "    for trainset, test in kfold.split(data):\n",
    "\n",
    "        train_data = pd.DataFrame(dat1[trainset], columns = [cols for cols in data.columns])\n",
    "        test_data = pd.DataFrame(dat1[test], columns = [cols for cols in data.columns])\n",
    "        #print('train: %s, test: %s' % (train_data, test_data))\n",
    "        #print(test_data.dtypes)\n",
    "\n",
    "        df_lessthan, df_morethan, y_actual, test_data, prior_prob, posterior_lessthan_nom, posterior_greaterthan_nom, posterior_lessthan_num, posterior_greaterthan_num = train(train_data, test_data)\n",
    "\n",
    "        y_pred, predicted_class, y_actual = predict(y_actual, test_data, prior_prob, posterior_lessthan_nom, posterior_greaterthan_nom, posterior_lessthan_num, posterior_greaterthan_num)\n",
    "\n",
    "\n",
    "        confusion_matrix, accuracy, F1score = evaluate(y_pred, predicted_class, y_actual)\n",
    "\n",
    "        total_recall += confusion_matrix[0][0]/(confusion_matrix[0][0] + confusion_matrix[0][1])\n",
    "        total_accuracy += accuracy\n",
    "        total_sensitivity += confusion_matrix[0][0]/(confusion_matrix[0][0] + confusion_matrix[0][1])\n",
    "        total_specificity += confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])\n",
    "        \n",
    "    print('k-folds = ', fold)\n",
    "    print('Average accuracy = ', total_accuracy/fold)\n",
    "    print('Average recall = ', total_recall/fold)\n",
    "    print('Average senstivity = ', total_sensitivity/fold)\n",
    "    print('Average Specificity = ', total_specificity/fold)\n",
    "    print('\\n')\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    total_recall = 0\n",
    "    total_sensitivity = 0\n",
    "    total_specificity = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Following are the results from the cross-validations - \n",
    " * 2-fold: Accuracy = 82%, Recall = 85%, Specificity = 70%\n",
    " * 10-fold: Accuracy = 81%, Recall = 84%, Specificity = 71%\n",
    " \n",
    "Looking at these metrics, it can be seen that the 2-fold model performs better in predicting the true positive cases whereas the 10-fold performs slightly better at prediciting the true negative cases. \n",
    "\n",
    "Overall, both the models perform similarly starting that for this dataset the size of the m-fold is sufficient at both m=2 and m=10 suggesting that both type of folds are providing enough data for the model to train and test. \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
